version: '3.8'

services:
  postgres:
    image: pgvector/pgvector:pg16
    container_name: where-postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: wherepass
      POSTGRES_DB: whereisthisplace
      POSTGRES_INITDB_ARGS: "--auth-host=md5"
    command: >
      bash -c "
      apt-get update && 
      apt-get install -y postgresql-16-postgis-3 && 
      docker-entrypoint.sh postgres 
      -c max_connections=200
      -c shared_buffers=256MB"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/01-init-db.sql
      - ./scripts/create-user.sql:/docker-entrypoint-initdb.d/02-create-user.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - whereisthisplace-network

  backend:
    image: where-backend-openai:latest
    container_name: where-backend-openai-gpu
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      PYTHONUNBUFFERED: "1"
      DATABASE_URL: ${DATABASE_URL:-postgresql://whereuser:wherepass@postgres:5432/whereisthisplace}
      TORCHSERVE_URL: ${TORCHSERVE_URL:-http://localhost:8080}
      LOG_LEVEL: ${LOG_LEVEL:-info}
      PYTHONPATH: /app
      TORCHSERVE_CONFIG_FILE: /app/config/config.properties
      MAPBOX_TOKEN: ${MAPBOX_TOKEN:-YOUR_MAPBOX_TOKEN_HERE}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_BASE_URL: ${OPENAI_BASE_URL:-https://api.openai.com/v1}
      RATE_LIMIT_REQUESTS: ${RATE_LIMIT_REQUESTS:-1000}
      RATE_LIMIT_PERIOD: ${RATE_LIMIT_PERIOD:-3600}
    volumes:
      - ./datasets:/app/datasets:rw
      - ./ml:/app/ml:rw
      - ./scripts:/app/scripts:rw
      - ./logs:/app/logs:rw
      - ./config:/app/config:ro
    ports:
      - "8000:8000"
      - "8080:8080"
      - "8081:8081"
      - "8082:8082"
    depends_on:
      postgres:
        condition: service_healthy
    entrypoint: /bin/bash
    command:
      - "-c"
      - |
        set -e
        echo '=== Backend Service Starting ==='
        echo "Running as user: $$(whoami)"
        
        # Debug environment variables
        echo "Environment variables:"
        echo "  PYTHONPATH: $$PYTHONPATH"
        echo "  LOG_LEVEL: $$LOG_LEVEL"
        echo "  TORCHSERVE_CONFIG_FILE: $$TORCHSERVE_CONFIG_FILE"
        
        echo 'Installing required Python packages...'
        /home/venv/bin/pip3 install --no-cache-dir "requests>=2.31.0" "fastapi>=0.100.0" "uvicorn>=0.23.0" "psycopg2-binary>=2.9.0" "asyncpg>=0.29.0" "python-multipart>=0.0.6" "python-dotenv>=1.0.0" "pgvector>=0.2.0" "aiohttp>=3.8.1"
        
        echo 'Starting TorchServe...'
        
        # Build TorchServe command
        TS_ARGS=("--start" "--ncs" "--model-store" "/model-store" "--models" "all")
        
        # Check for config file
        if [ -f "$$TORCHSERVE_CONFIG_FILE" ]; then
          echo "INFO: Using TorchServe config: $$TORCHSERVE_CONFIG_FILE"
          TS_ARGS+=("--ts-config" "$$TORCHSERVE_CONFIG_FILE")
        else
          echo "WARNING: Config file $$TORCHSERVE_CONFIG_FILE not found, using defaults"
        fi
        
        # Start TorchServe
        torchserve "$${TS_ARGS[@]}"
        
        echo "Contents of /model-store:"
        ls -la /model-store/
        
        echo 'Waiting for TorchServe to initialize...'
        sleep 30
        
        # Check TorchServe health
        if command -v curl &> /dev/null; then
          echo "Checking TorchServe health..."
          curl -s http://localhost:8081/ping || echo "TorchServe ping failed"
          curl -s http://localhost:8081/models || echo "TorchServe models query failed"
        fi
        
        echo 'Starting FastAPI application...'
        cd /app
        
        # Set log level with fallback
        UVICORN_LOG_LEVEL="$${LOG_LEVEL:-info}"
        echo "Starting Uvicorn with log level: $$UVICORN_LOG_LEVEL"
        
        # Start Uvicorn
        /home/venv/bin/python3 -m uvicorn api.main:app \
          --host 0.0.0.0 \
          --port 8000 \
          --log-level "$$UVICORN_LOG_LEVEL"
    networks:
      - whereisthisplace-network

  adminer:
    image: adminer
    container_name: where-adminer
    ports:
      - "8090:8080"
    depends_on:
      - postgres
    networks:
      - whereisthisplace-network
    environment:
      - ADMINER_DEFAULT_SERVER=postgres

volumes:
  postgres_data:
    driver: local

networks:
  whereisthisplace-network:
    driver: bridge
